#!/usr/bin/python

import sys
import termios, fcntl, struct
import webbrowser
import HTMLParser
import operator
import re

from urllib import quote_plus
from httplib import HTTPConnection
from getopt import getopt, GetoptError
#from HTMLParser import HTMLParser
import sys
reload(sys)
sys.setdefaultencoding("utf-8")
# Global variables
columns  = None    # Terminal window size.
start    = None    # The first result to display (option -s)
num      = None    # Number of results to display (option -n)
lang     = None    # Language to search for (option -l)
openUrl  = False   # If True, opens the first URL in browser (option -j)
colorize = False   # If True, colorizes the output (option -C)

word_length = 1
key_word = ""

title_hash = {}

pattern_match = []
global_pattern_match = {}

rm_word_list = ['','in','at','a','is','was','the','to','from','of','on','and','-','|','an','by','for']

i_pattern = ["*** and ","*** is ", "*** was","*** were","*** "] #actually the last one contains everything above
pattern = []

global_frequent_word_list = {}
frequent_word_list = {}

seed_word_list = []

global_combined_seed_pattern = {}
combined_seed_pattern = []


# Classes
class GoogleParser(HTMLParser.HTMLParser):
    def __init__(self):
        HTMLParser.HTMLParser.__init__(self)
        self.handle_starttag = self.main_start
        self.handle_data = self.main_data
        self.handle_endtag = self.main_end
    def main_start(self, tag, attrs):
        if tag == "li" and len(attrs) > 0 and attrs[0] == ("class", "g"):
            self.title = ""
            self.url   = ""
            self.text  = ""
            self.handle_starttag = self.li_start
            self.handle_data = self.li_data
            self.handle_endtag = self.li_end
    def main_data(self, data):
        pass
    def main_end(self, tag):
        pass
    # <li class="g"> ... </li>
    def li_start(self, tag, attrs):
        if tag == "h3":
            self.handle_starttag = self.h3_start
            self.handle_data = self.h3_data
            self.handle_endtag = self.h3_end
        elif tag == "div":
            self.handle_starttag = self.div_start
            self.handle_data = self.div_data
            self.handle_endtag = self.div_end
    def li_data(self, data):
        pass
    def li_end(self, tag):
        if tag == "div":
            parse_google_entry(self.title, self.url, self.text)
            self.handle_starttag = self.main_start
            self.handle_data = self.main_data
            self.handle_endtag = self.main_end
    # <h3> ... </h3>
    def h3_start(self, tag, attrs):
        if tag == "a":
            self.url = attrs[0][1]
    def h3_data(self, data):
        self.title += data
    def h3_end(self, tag):
        if tag == "h3": 
            self.handle_starttag = self.li_start
            self.handle_data = self.li_data
            self.handle_endtag = self.li_end
    # <div> ... </div>
    def div_start(self, tag, start):
        if tag == "br":
            self.handle_starttag = self.li_start
            self.handle_data = self.li_data
            self.handle_endtag = self.li_end
    def div_data(self, data):
        self.text += data
    def div_end(self, tag):
        pass

class MLStripper(HTMLParser.HTMLParser):
    def __init__(self):
        self.reset()
        self.fed = []
    def handle_data(self, d):
        self.fed.append(d)
    def get_data(self):
        return ''.join(self.fed)

def strip_tags(html):
    s = MLStripper()
    s.feed(html)
    return s.get_data()

# Functions
def rm_empty_lines(txt):
    while '\n\n' in txt:
        txt=txt.replace('\n\n','\n')
    while '  ' in txt:
        txt=txt.replace('  ','zzz')
    return txt

def rm_line_if_length_is_short(txt):
    list_txt = txt.split('\n')
    new = []
    for ele in list_txt:
	if len(ele) > 40:	
		new.append(ele)
    new_txt = "\n".join(new)
    return new_txt

def pattern_match_web_page(txt):
    splited = re.split('[.|,|;|\n]',txt)
    for sen in splited:
        #print sen + "]]]]]]]]]]]]]]]]"
        match_pattern(sen)
        hash_word(sen)
        match_combined_seedword(sen)

def read_web_page(url):
    import urllib
    conn = urllib.urlopen(url)
    html = conn.read()
    #print html
    text = strip_tags(html)
    #filtered = filter(lambda x: not re.match(r'^\s*$', x),text)
    filtered = rm_empty_lines(text)
    filtered = rm_line_if_length_is_short(text)
    #print filtered[0:20000]
    print "[read_web_page]The length of useful info is " + str(len(filtered))    
    return filtered.lower()

def match_combined_seedword(input):
    global seed_word_list
    global combined_seed_pattern
    count = 0
    for group in seed_word_list:
        for seed in group: 
            if seed in input:
                count +=1
                break
    if count >=2:
        combined_seed_pattern.append(input)
        

def init_pattern(word_list):
    global pattern
    global i_pattern
    pattern = []
    aword = " ".join(word_list) 
    dum_list = []
    for x in i_pattern:
	dum_list.append(x.replace("***",aword))
    if len(word_list) >=2:
        for word in word_list:
		for x in i_pattern:
			dum_list.append(x.replace("***",word))
    pattern = dum_list
    print "[init_pattern] The patterns are for word: " + str(aword)
    print pattern

def match_pattern(input):
    global pattern
    for x in pattern:
	if (x in input):
		pattern_match.append(input)
		return

def succinct_pattern():
    global pattern_match
    global frequent_word_list
    new_pattern = []
    check = frequent_word_list[1]
    for p in pattern_match:
        l = p.split(' ')
        n_l = []
        for tup in l:
            if tup in check:
                n_l.append(tup)
        new_p = ' '.join(n_l)
        new_pattern.append(new_p)
    pattern_match = new_pattern
    print pattern_match

def clean_input(input):
    line = re.sub('[. !,$@|:]','',input)
    return line


def rm_useless_word(input):
   global rm_word_list
   return False
   if input in rm_word_list:
	return True
   return False

def check_frequency(word_list):
    global word_length
    global frequent_word_list
    if word_length <=1:
	return True
    else:
	check_hash = frequent_word_list[word_length-1]
	for word in word_list:
		t = (clean_input(word)).lower()
                #print "t is :" + t
    #            print "word in check frequency is " + t
		if t not in check_hash:
                 #       print "not in check_hash:" + t
			return False
        return True

def hash_word(info):
    global word_length
    word_list = info.split()
    for i in range(0,len(word_list)):
	#print "word is " + word
        word = word_list[i:i+word_length]
        #print "word list in hash_title"
        #print word
        if check_frequency(word) == False:
		continue
        else:
		clean_word_list = []
		flag = 0
                for x in word:
			c = (clean_input(x)).lower()
			if rm_useless_word(c) == True:
				flag = 1
			else:
				clean_word_list.append(c)
		if flag == 1:
			continue	
 		final = " ".join(clean_word_list)
        	#if word_length > 1:
		#	print "Found:" + final
		if final in title_hash:
			title_hash[final] +=1
		else:
			title_hash[final] = 1

def limit_hash():
#    stop_index = 0
    stored_key = []
    for key in title_hash:
	if title_hash[key] < 3:
		stored_key.append(key)

    for key in stored_key:
	title_hash.pop(key,None)

def parse_general_web(url):
    txt = read_web_page(url)
    #word_length = 1
    pattern_match_web_page(txt)
    
def pattern_mining():
    return ""

def overlapping_word_list():
    return ""

def parse_google_html(parser,content):
    global word_length
    global title_hash
    parser.feed(content)
    limit_hash()
    frequent_word_list[word_length] = title_hash
    title_hash = {}

def parse_google_entry(title, url, text):
    # Open the URL in a web browser if option -j was specified.
    if openUrl:
        webbrowser.open(url)
        sys.exit(0)
    # Print the title and the URL.
    if colorize:
        print "\x1B[96m* \x1B[92m%s\n\x1B[93m%s\x1B[39m" % (title, url)
    else:
        try:
                #print "url is " + url
		hash_word(title);
               # match_pattern(title.lower())
	except:
		print "[parse_google_entry]Error happens in print statement"
    hash_word(text)
    try:
        index = url.index("&sa=")
        url = url[0:index]
        if url.startswith('/url?q='):
            url = url[7:]
            print "[parse_google_entry]begin parse url:" + url
            parse_general_web(url)
        else:
            print "url is not valid"
    except:
        print "url not valid"
  #  match_pattern(text.lower())

def usage():
    print "Usage: google [OPTIONS] KEYWORDS..."
    print "Performs a Google search and prints the results to stdout.\n"
    print "Options"
    print "  -s N     start at the Nth result"
    print "  -n N     show N results"
    print "  -l LANG  display in language LANG, such as fi for Finnish"
    print "  -C       enable color output"
    print "  -j       open the first result in a web browser\n"
    sys.exit(1)

def search_google_with_seedword(seedword,url):
    global word_length
    # Construct the query URL.
    url += "q="
    word_list = seedword.split(' ')
    #print "[search google with seedword]seedword is " + seedword
    url += quote_plus(word_list[0])
    if len(word_list) > 1:
        for kw in word_list[1:]:
     #       print "kw is " + kw
            url += "+" + quote_plus(kw)
    #Init pattern for a certain seed word
    init_pattern(word_list)
    print "[search google with seedword] url is : " + str(url)
    # Connect to Google and request the result page.
    conn = HTTPConnection("www.google.com")
    conn.request("GET", url)
    resp = conn.getresponse()
    if resp.status != 200:
        # The server responded with an error.
        print "Server responded with an error:", str(resp.status)
        sys.exit(1)

    # Parse the HTML document and print the results.
    save = resp.read()
    parser = GoogleParser()
    parse_google_html(parser,save)#build frequency for single word
    #word_length +=1
    #parse_google_html(parser,save)#build frequency for double word


def begin_search_google(seed_word_list,url):
    global pattern_match
    global frequent_word_list
    global combined_seed_pattern
    global global_pattern_match
    global global_frequent_word_list
    global global_combined_seed_pattern
    for word in seed_word_list:
        print "[begin_search_google]Seedword: "+ str(word) +"==============================="
        # Init the global variable before search begins
        init_global_variable_before_google_search()
 
        search_google_with_seedword(word,url)
        #Save the pattern
        global_pattern_match[word] = pattern_match
        #Save frequent word in html
        global_frequent_word_list[word] = frequent_word_list
        global_combined_seed_pattern[word] = combined_seed_pattern
        print "[begin_search_google] Combined seed pattern:"
        print combined_seed_pattern
        print "Frequent pattern list:"
        #print frequent_word_list
        #print "Pattern match:"
        #print pattern_match
        

def init_global_variable_before_google_search():
    global pattern_match
    global word_length
    global frequent_word_list
    global combined_seed_pattern
    word_length = 1
    pattern_match = []
    frequent_word_list = {}
    combined_seed_pattern = []    
 
def init_seed_word_list():
    global seed_word_list
    global keywords
    for x in keywords:
        name_group = []
        name_group.append(x)
        s = x.split(' ')
        if len(s) > 1:
            for t in s:
                name_group.append(t)
        seed_word_list.append(name_group)
    print "[init_seed_word_list] seed word list is:"
    print seed_word_list

def print_info():
    global keywords
    global global_combined_seed_pattern
    for x in keywords:
        print "Print info with keyword: " + str(x) + "============================================="
        print "Combined seed pattern:"
        print global_combined_seed_pattern[x]

########### Program Main

# Process command line options.
optlist = None
keywords = None

if len(sys.argv) < 2:
    usage()

try:
    optlist, keywords = getopt(sys.argv[1:], "s:n:l:Cj")
    for opt in optlist:
        if opt[0] == "-s":
            # Option -s N
            if not opt[1].isdigit():
                print "google: option -s needs an integer"
                sys.exit(1)
            start = opt[1]
        elif opt[0] == "-n":
            # Option -n N
            if not opt[1].isdigit():
                print "google: option -n needs an integer"
                sys.exit(1)
            num = opt[1]
        elif opt[0] == "-l":
            # Option -l LANG
            lang = opt[1]
        elif opt[0] == "-C":
            # Option -C
            colorize = True
        elif opt[0] == "-j":
            # Option -j
            openUrl = True
    if len(keywords) < 1:
        usage()
except GetoptError, e:
    print "google:", e
    sys.exit(1)

# Construct the query URL.
url = "/search?"

if start != None:
    url += "start=" + start + "&"
if num != None:
    url += "num=" + num + "&"
if lang != None:
    url += "hl=" + lang + "&"
"""
url += "q=" + quote_plus(keywords[0])
url += "q=" + quote_plus(keywords[0])
for kw in keywords[1:]:
    url += "+" + quote_plus(kw)
"""
#key_word = ' '.join(keywords)
for x in keywords:
    print "seed word: " + str(x)

init_seed_word_list() 
begin_search_google(keywords,url)

print_info()

#init_pattern()
"""
if not openUrl:
    print "\nhttp://www.google.com%s\n" % url

# Get the terminal window size.
winsz = fcntl.ioctl(sys.stdout, termios.TIOCGWINSZ, "1234")
columns = struct.unpack("HH", winsz)[1]

# Connect to Google and request the result page.
conn = HTTPConnection("www.google.com")
conn.request("GET", url)
print "url is " + str(url)
resp = conn.getresponse()
if resp.status != 200:
    # The server responded with an error.
    print "Server responded with an error:", str(resp.status)
    sys.exit(1)

# Parse the HTML document and print the results.
save = resp.read()
parser = GoogleParser()
parse_google_html(parser,save)
word_length +=1
#print check_frequency(['roger','federer'])
parse_google_html(parser,save)

print "Frequent pattern list:"
print frequent_word_list
print "Pattern match:"
print pattern_match
#parser.feed(resp.read())
conn.close()
wiki = "http://en.wikipedia.org/wiki/Rafael_Nadal"
txt = read_web_page(wiki)
word_length = 1
pattern_match_web_page(txt)
#print "key word is " + ' '.join( keywords)
#print_title_hash()
print "!!!!!!!!!!!!!!!!!!!!!pattern match length is"
print len(pattern_match)
"""
#succinct_pattern()
# vim:set sts=4 sw=4 et:
