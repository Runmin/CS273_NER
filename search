#!/usr/bin/python

import sys
import termios, fcntl, struct
import webbrowser
import HTMLParser
import operator
import re

from urllib import quote_plus
from httplib import HTTPConnection
from getopt import getopt, GetoptError
#from HTMLParser import HTMLParser
import sys
reload(sys)
sys.setdefaultencoding("utf-8")
# Global variables
columns  = None    # Terminal window size.
start    = None    # The first result to display (option -s)
num      = None    # Number of results to display (option -n)
lang     = None    # Language to search for (option -l)
openUrl  = False   # If True, opens the first URL in browser (option -j)
colorize = False   # If True, colorizes the output (option -C)

word_length = 1
key_word = ""

title_hash = {}

pattern_match = []
global_pattern_match = {}

rm_word_list = ['','in','at','a','is','was','the','to','from','of','on','and','-','|','an','by','for']

i_pattern = ["*** and ","*** is ", "*** was","*** were","*** "] #actually the last one contains everything above
pattern = []

global_frequent_word_list = {}
frequent_word_list = {}

seed_word_list = []

global_combined_seed_pattern = {}
combined_seed_pattern = []

global_saved_page_info = []
# Classes
class GoogleParser(HTMLParser.HTMLParser):
    def __init__(self):
        HTMLParser.HTMLParser.__init__(self)
        self.handle_starttag = self.main_start
        self.handle_data = self.main_data
        self.handle_endtag = self.main_end
    def main_start(self, tag, attrs):
        if tag == "li" and len(attrs) > 0 and attrs[0] == ("class", "g"):
            self.title = ""
            self.url   = ""
            self.text  = ""
            self.handle_starttag = self.li_start
            self.handle_data = self.li_data
            self.handle_endtag = self.li_end
    def main_data(self, data):
        pass
    def main_end(self, tag):
        pass
    # <li class="g"> ... </li>
    def li_start(self, tag, attrs):
        if tag == "h3":
            self.handle_starttag = self.h3_start
            self.handle_data = self.h3_data
            self.handle_endtag = self.h3_end
        elif tag == "div":
            self.handle_starttag = self.div_start
            self.handle_data = self.div_data
            self.handle_endtag = self.div_end
    def li_data(self, data):
        pass
    def li_end(self, tag):
        if tag == "div":
            parse_google_entry(self.title, self.url, self.text)
            self.handle_starttag = self.main_start
            self.handle_data = self.main_data
            self.handle_endtag = self.main_end
    # <h3> ... </h3>
    def h3_start(self, tag, attrs):
        if tag == "a":
            self.url = attrs[0][1]
    def h3_data(self, data):
        self.title += data
    def h3_end(self, tag):
        if tag == "h3": 
            self.handle_starttag = self.li_start
            self.handle_data = self.li_data
            self.handle_endtag = self.li_end
    # <div> ... </div>
    def div_start(self, tag, start):
        if tag == "br":
            self.handle_starttag = self.li_start
            self.handle_data = self.li_data
            self.handle_endtag = self.li_end
    def div_data(self, data):
        self.text += data
    def div_end(self, tag):
        pass

class MLStripper(HTMLParser.HTMLParser):
    def __init__(self):
        self.reset()
        self.fed = []
    def handle_data(self, d):
        self.fed.append(d)
    def get_data(self):
        return ''.join(self.fed)

def strip_tags(html):
    s = MLStripper()
    s.feed(html)
    return s.get_data()

# Functions
def rm_empty_lines(txt):
    while '\n\n' in txt:
        txt=txt.replace('\n\n','\n')
    while '  ' in txt:
        txt=txt.replace('  ','zzz')
    return txt

def rm_line_if_length_is_short(txt):
    list_txt = txt.split('\n')
    new = []
    for ele in list_txt:
	if len(ele) > 40:	
		new.append(ele)
    new_txt = "\n".join(new)
    return new_txt

def rm_parenthesis(txt):
    new_txt = re.sub(r'\[.*\]',"",txt)
    txt = re.sub(r'\(.*\)',"",new_txt)
    return txt

def pattern_match_web_page(txt):
    splited = re.split('[.|,|;|\n]',txt)
    for sen in splited:
        #print sen + "]]]]]]]]]]]]]]]]"
        match_pattern(sen)
        #hash_word(sen)
    #    match_combined_seedword(sen)

def read_web_page(url):
    import urllib
    conn = urllib.urlopen(url)
    html = conn.read()
    #print html
    filtered = strip_tags(html)
    #filtered = filter(lambda x: not re.match(r'^\s*$', x),text)
    filtered = rm_parenthesis(filtered)
    filtered = rm_empty_lines(filtered)
    filtered = rm_line_if_length_is_short(filtered)
    #filtered = rm_parenthesis(filtered)
    #print filtered[0:20000]
    print "[read_web_page]The length of useful info is " + str(len(filtered))    
    return filtered.lower()

def match_combined_seedword(input):
    global seed_word_list
    global combined_seed_pattern
    count = 0
    for group in seed_word_list:
        for seed in group: 
            if seed in input:
                count +=1
                break
    if count >=2:
        combined_seed_pattern.append(input)
        

def init_pattern(word_list):
    global pattern
    global i_pattern
    pattern = []
    aword = " ".join(word_list) 
    dum_list = []
    for x in i_pattern:
	dum_list.append(x.replace("***",aword))
    if len(word_list) >=2:
        for word in word_list:
		for x in i_pattern:
			dum_list.append(x.replace("***",word))
    pattern = dum_list
    print "[init_pattern] The patterns are for word: " + str(aword)
    print pattern

def match_pattern(info):
    global pattern
    global pattern_match
    for x in pattern:
	if (x in info):
		pattern_match.append(info)
		return

def succinct_pattern():
    global pattern_match
    global frequent_word_list
    new_pattern = []
    check = frequent_word_list[1]
    for p in pattern_match:
        l = p.split(' ')
        n_l = []
        for tup in l:
            if tup in check:
                n_l.append(tup)
        new_p = ' '.join(n_l)
        new_pattern.append(new_p)
    pattern_match = new_pattern
    print pattern_match

def clean_input(input):
    line = re.sub('[. !,$@|:]','',input)
    return line


def rm_useless_word(input):
   global rm_word_list
   return False
   if input in rm_word_list:
	return True
   return False

def check_frequency(word_list):
    global word_length
    global frequent_word_list
    if word_length <=1:
	return True
    else:
	check_hash = frequent_word_list[word_length-1]
	for word in word_list:
		t = (clean_input(word)).lower()
                #print "t is :" + t
    #            print "word in check frequency is " + t
		if t not in check_hash:
                 #       print "not in check_hash:" + t
			return False
        return True

def stat_word_frequency(list_word):
    global title_hash
    global word_length
    word_length = 1
    title_hash = {}
    for ele in list_word:
        hash_word(ele)
    limit_hash()
    return title_hash

def hash_word(info):
    global word_length
    word_list = info.split()
    for i in range(0,len(word_list)):
	#print "word is " + word
        word = word_list[i:i+word_length]
        #print "word list in hash_title"
        #print word
        if check_frequency(word) == False:
		continue
        else:
		clean_word_list = []
		flag = 0
                for x in word:
			c = (clean_input(x)).lower()
			if rm_useless_word(c) == True:
				flag = 1
			else:
				clean_word_list.append(c)
		if flag == 1:
			continue	
 		final = " ".join(clean_word_list)
        	#if word_length > 1:
		#	print "Found:" + final
		if final in title_hash:
			title_hash[final] +=1
		else:
			title_hash[final] = 1

def limit_hash():
#    stop_index = 0
    global title_hash
    stored_key = []
    for key in title_hash:
	if title_hash[key] < 3:
		stored_key.append(key)

    for key in stored_key:
	title_hash.pop(key,None)

def parse_general_web(url): 
    txt = read_web_page(url)
    #word_length = 1
    pattern_match_web_page(txt)
    
def pattern_mining():
    return ""

def overlapping_word_list():
    return ""

def parse_google_html(parser,content):
    global word_length
    global title_hash
    parser.feed(content)
    limit_hash()
    frequent_word_list[word_length] = title_hash
    title_hash = {}

def parse_google_entry(title, url, text):
    # Open the URL in a web browser if option -j was specified.
    if openUrl:
        webbrowser.open(url)
        sys.exit(0)
    # Print the title and the URL.
    if colorize:
        print "\x1B[96m* \x1B[92m%s\n\x1B[93m%s\x1B[39m" % (title, url)
    else:
        try:
                print "url is " + url
	#	hash_word(title);
               # match_pattern(title.lower())
	except:
		print "[parse_google_entry]Error happens in print statement"
    hash_word(text)
    try:
        index = url.index("&sa=")
        url = url[0:index]
        if url.startswith('/url?q='):
            url = url[7:]
            print "[parse_google_entry]begin parse url:" + url
            parse_general_web(url)
        else:
            print "url is not valid"
    except:
        print "url not valid"
  #  match_pattern(text.lower())

def usage():
    print "Usage: google [OPTIONS] KEYWORDS..."
    print "Performs a Google search and prints the results to stdout.\n"
    print "Options"
    print "  -s N     start at the Nth result"
    print "  -n N     show N results"
    print "  -l LANG  display in language LANG, such as fi for Finnish"
    print "  -C       enable color output"
    print "  -j       open the first result in a web browser\n"
    sys.exit(1)

def flat_dict_into_list(dict_input):
    list_output = []
    for key in dict_input:
        for sen in dict_input[key]:
            list_output.append(sen)
    return list_output

def flat_dictkey_into_list(dict_input):
    dum = []
    for key in dict_input:
        dum.append(key)
    return dum

def update_dict_with_key(hashmap,key):
    #print "[update_dict_with_key] key is " + key
    if key in hashmap:
        hashmap[key] +=1
    else:
        hashmap[key] = 1

def limit_hash_result(hashresult,limit):
    temp_hash = {}
    for key in hashresult:
        if hashresult[key]  >= limit:
            temp_hash[key] = hashresult[key]
    return temp_hash

def prepare_list(word_list):
    new_list = []
    for x in word_list:
        t = x.split(' ')
        new_list.append(t)
    return new_list

def print_pattern_with_star(sen_list):
    for sen in sen_list:
        if '*' in sen:
            print sen

def frequent_pattern_mining(): 
    global global_pattern_match
    global keywords
    for name in keywords:
        move_name_into_stars(name, global_pattern_match)
#    for key in global_pattern_match:
#        print "key is " + key + "================================"
#        print global_pattern_match[key]
    data_list = flat_dict_into_list(global_pattern_match)
#    print "data_list is ==============================================="
#    print data_list
    single_frequent_hash = get_single_frequent_hash(data_list)
    print single_frequent_hash
    dum = []
    frequent_list =[] 
    for key in single_frequent_hash:
        frequent_list.append([key])
    dum = do_frequent_pattern_mining(frequent_list,data_list)
    dum = prepare_list(dum)
    print dum

    #dum = do_frequent_pattern_mining(dum,data_list)
    #dum = prepare_list(dum)
    #print dum
    #print_pattern_with_star(dum)
    #for i in range(1,3):
 
def get_single_frequent_hash(data_list):
    temp = {}
    for sen in data_list:
        dum = sen.split(' ')
        for word in dum:
            update_dict_with_key(temp,word)
    temp = limit_hash_result(temp,20)
    return temp

def get_selection_element_list(input_list,total):
    s_list = []
    result = []
    length = len(input_list)
    
    if len(input_list) == 1:
        return input_list
    do_get_selection_element_list(input_list,s_list,0,0,total,result)
    return result

def copy_list(l1,l2):
    for x in l1:
        l2.append(x)


def do_get_selection_element_list(input_list,s_list,index,num_ele,total,result):
    #print "index = " + str(index) + "num_ele "
    if num_ele == total and index <= len(input_list):
     #   print "found!!!"
     #   print s_list
        result.append(s_list)
    elif index >=len(input_list):
        return
    else:
    #    print "s_list with index = " + str(index)
    #    print s_list
        save1 = []
        save2 = []
        copy_list(s_list,save1)
        copy_list(s_list,save2)
        do_get_selection_element_list(input_list,save1,index+1, num_ele,total,result)
        save2.append(input_list[index])
        do_get_selection_element_list(input_list,save2,index+1,num_ele+1,total,result)
        return

def merge_two_list(l1,l2):
    dum = []
    copy_list(l1,dum)
    for x in l2:
        if x not in dum:
            dum.append(x)
    return dum

def check_all_sub_frequency(input_list,frequent_list):
    all_sub = get_selection_element_list(input_list,len(input_list)-1)
   # print "all sub"
   # print all_sub
    for x in all_sub:
        if x not in frequent_list:
            return False
    return True


def do_frequent_pattern_mining(frequent_list,data_list):
    dict_count = {} 
    #frequent_list [['a'],['b']]
    for i in range(0,len(frequent_list)):
        for j in range(i,len(frequent_list)):
            p1 = frequent_list[i]
            p2 = frequent_list[j]
            p = merge_two_list(p1,p2)
            if check_all_sub_frequency(p,frequent_list):
           #     print "true"      
                hash_frequency(p,data_list,dict_count)               
    temp = limit_hash_result(dict_count,10)    
    new_fre_list = flat_dictkey_into_list(temp)
    return new_fre_list

def merge_list_into_key(item_list):
    key = " ".join(item_list)
    return key    

def hash_frequency(potential_list, data_list, dict_count):
    length = len(potential_list)
    for sen in data_list:
        count = 0
        for item in potential_list:
            if item in sen:
                count +=1
        if count == length:
            key = merge_list_into_key(potential_list)
            update_dict_with_key(dict_count,key)

def search_google_with_seedword(seedword,url):
    global word_length
    # Construct the query URL.
    url += "q="
    word_list = seedword.split(' ')
    #print "[search google with seedword]seedword is " + seedword
    url += quote_plus(word_list[0])
    if len(word_list) > 1:
        for kw in word_list[1:]:
     #       print "kw is " + kw
            url += "+" + quote_plus(kw)
    #Init pattern for a certain seed word
    init_pattern(word_list)
    print "[search google with seedword] url is : " + str(url)
    # Connect to Google and request the result page.
    conn = HTTPConnection("www.google.com")
    conn.request("GET", url)
    resp = conn.getresponse()
    if resp.status != 200:
        # The server responded with an error.
        print "Server responded with an error:", str(resp.status)
        sys.exit(1)

    # Parse the HTML document and print the results.
    save = resp.read()
    parser = GoogleParser()
    parse_google_html(parser,save)#build frequency for single word
    #word_length +=1
    #parse_google_html(parser,save)#build frequency for double word


def begin_search_google(seed_list,url):
    global pattern_match
    global frequent_word_list
    global combined_seed_pattern
    global global_pattern_match
    global global_frequent_word_list
    global global_combined_seed_pattern
    for word in seed_list:
        print "[begin_search_google]Seedword: "+ str(word) +"==============================="
        # Init the global variable before search begins
        init_global_variable_before_google_search()
 
        search_google_with_seedword(word,url)
        #Save the pattern
        global_pattern_match[word] = pattern_match
        #Save frequent word in html
        global_frequent_word_list[word] = frequent_word_list
        global_combined_seed_pattern[word] = combined_seed_pattern
        

def init_global_variable_before_google_search():
    global pattern_match
    global word_length
    global frequent_word_list
    global combined_seed_pattern
    word_length = 1
    pattern_match = []
    frequent_word_list = {}
    combined_seed_pattern = []    
 
def init_seed_word_list():
    global seed_word_list
    global keywords
    for x in keywords:
        name_group = []
        name_group.append(x)
        s = x.split(' ')
        if len(s) > 1:
            for t in s:
                name_group.append(t)
        seed_word_list.append(name_group)
    print "[init_seed_word_list] seed word list is:"
    print seed_word_list

def print_info():
    global keywords
    global global_combined_seed_pattern
    for x in keywords:
        print "Print info with keyword: " + str(x) + "============================================="
        print "Combined seed pattern:"
        print global_combined_seed_pattern[x]

def flat_list(dic):
    flat = []    
    for x in dic:
        for y in dic[x]:
            flat.append(y)
    return flat
 
def reduce_list_with_frequent_word(list_pattern, stat_freq):
    new_pattern = []
    for x in list_pattern:
        d = x.split(" ")
        new = []
        for ele in d:
            if ele in stat_freq:
                new.append(ele)
        new_pattern.append(" ".join(new))
    return new_pattern

def return_closer_pattern(pattern_list):
    closer_pattern = []
    for p in pattern_list:
        p_list = p.split(" ")
      #  print p
        start = -1
        end = -1
        for i in range(0,len(p_list)):
            if p_list[i] == "*":
                if start == -1:
                    start = i
             #       print "start = " + str(start)
                else:
                    end = i
                    break
        if end - start > 0 and end -start <= 5:
            closer_pattern.append(p)
            #print "append!"
    return closer_pattern

def calculate_overlapping():
    #use bag of word model
    #first, change name into * 
    global keywords
    global global_combined_seed_pattern
    overlap_pattern = []
   # print global_combined_seed_pattern
    for x in keywords:
        #print "Print info with keyword: " + str(x) + "============================================="
        #print "Combined seed pattern:"
        print "name is "
        print x
        move_name_into_stars(x,global_combined_seed_pattern)
   # print global_combined_seed_pattern
    #import itertools
    merged = flat_list(global_combined_seed_pattern)#list(itertools.chain(*global_combined_seed_pattern))
   # print "merged"
   # print merged
    #word_fre_list = stat_word_frequency(merged)
    #print word_fre_list
    #new_pattern = reduce_list_with_frequent_word(merged, word_fre_list)
    #print new_pattern
    closer = return_closer_pattern(merged)
    print "closer:"
    print closer
"""    
    for i in range(0,len(merged)):
        if i == len(merged)-1:
            break
        for j in range(i+1,len(merged)):
            #print "i:" + str(merged[i])
            #print "j:" + str(merged[j])
            check = merged[i].split(" ")
            against = merged[j].split(" ")
            count = 0
            if len(merged[i]) > len(merged[j]):
                save = check
                check = against
                against = save
            for ele in check:
                if ele in against and ele != "*":
                    count+=1
            if count > len(check) /2 :
                dum = [" ".join(check)," ".join(against)]
                overlap_pattern.append(dum)
    print "[calculate overlapping]"
    for x in overlap_pattern:
        print x
"""
 
def sentence_overlap(s1,s2):
    return ""

def move_name_into_stars(name_seed, target):
    global seed_word_list
    global keywords
    #global global_combined_seed_pattern
    print "target type is " + str(type(target))
    new_pattern = []
    pattern = target[name_seed]

    for p in pattern:
        for x in keywords:
            if x in p:
                p = p.replace(x,"*")
        new_pattern.append(p) 
         
    check_list = []
    for group in seed_word_list:
        for name in group:
            check_list.append(name)

    pattern = []
    for p in new_pattern:
        for x in check_list:
            if x in p:
                p = p.replace(x,"*")
        pattern.append(p)
    target[name_seed] = pattern
    #print pattern
   
    
########### Program Main
# Process command line options.
optlist = None
keywords = None

if len(sys.argv) < 2:
    usage()

try:
    optlist, keywords = getopt(sys.argv[1:], "s:n:l:Cj")
    for opt in optlist:
        if opt[0] == "-s":
            # Option -s N
            if not opt[1].isdigit():
                print "google: option -s needs an integer"
                sys.exit(1)
            start = opt[1]
        elif opt[0] == "-n":
            # Option -n N
            if not opt[1].isdigit():
                print "google: option -n needs an integer"
                sys.exit(1)
            num = opt[1]
        elif opt[0] == "-l":
            # Option -l LANG
            lang = opt[1]
        elif opt[0] == "-C":
            # Option -C
            colorize = True
        elif opt[0] == "-j":
            # Option -j
            openUrl = True
    if len(keywords) < 1:
        usage()
except GetoptError, e:
    print "google:", e
    sys.exit(1)

# Construct the query URL.
url = "/search?"

if start != None:
    url += "start=" + start + "&"
if num != None:
    url += "num=" + num + "&"
if lang != None:
    url += "hl=" + lang + "&"
"""
url += "q=" + quote_plus(keywords[0])
url += "q=" + quote_plus(keywords[0])
for kw in keywords[1:]:
    url += "+" + quote_plus(kw)
"""
#key_word = ' '.join(keywords)
for x in keywords:
    print "seed word: " + str(x)

init_seed_word_list() 
begin_search_google(keywords,url)
frequent_pattern_mining()

#input_list = ['a','b','c','d','e']
#d = get_selection_element_list(input_list,3)
#print d

#print_info()
#calculate_overlapping()
#init_pattern()
"""
if not openUrl:
    print "\nhttp://www.google.com%s\n" % url

# Get the terminal window size.
winsz = fcntl.ioctl(sys.stdout, termios.TIOCGWINSZ, "1234")
columns = struct.unpack("HH", winsz)[1]

# Connect to Google and request the result page.
conn = HTTPConnection("www.google.com")
conn.request("GET", url)
print "url is " + str(url)
resp = conn.getresponse()
if resp.status != 200:
    # The server responded with an error.
    print "Server responded with an error:", str(resp.status)
    sys.exit(1)

# Parse the HTML document and print the results.
save = resp.read()
parser = GoogleParser()
parse_google_html(parser,save)
word_length +=1
#print check_frequency(['roger','federer'])
parse_google_html(parser,save)

print "Frequent pattern list:"
print frequent_word_list
print "Pattern match:"
print pattern_match
#parser.feed(resp.read())
conn.close()
wiki = "http://en.wikipedia.org/wiki/Rafael_Nadal"
txt = read_web_page(wiki)
word_length = 1
pattern_match_web_page(txt)
#print "key word is " + ' '.join( keywords)
#print_title_hash()
print "!!!!!!!!!!!!!!!!!!!!!pattern match length is"
print len(pattern_match)
"""
#succinct_pattern()
# vim:set sts=4 sw=4 et:
